---
title: 'Autonomous Navigation'
sidebar_label: 'Autonomous Navigation'
sidebar_position: 6
---

import LinkButton from '@site/src/components/LinkButton';
import LiteYouTubeEmbed from 'react-lite-youtube-embed';
import ImageZoom from '@site/src/components/ImageZoom';

In this tutorial, we will show you how to perform SLAM (Simultaneous
localization and mapping) and autonomous navigation on Leo Rover equipped with
IMU and LiDAR sensors.

## Prerequisites

<LinkButton to="/docs/leo-rover/guides/ssh" description="Connect via SSH" />

<LinkButton
  to="/docs/leo-rover/guides/software-update"
  description="Software update"
/>

<LinkButton
  to="/docs/leo-rover/guides/firmware-update"
  description="Firmware update"
/>

<LinkButton
  to="install-ros-on-your-computer"
  description="Install ROS on your computer"
/>

<LinkButton to="ros-development" description="ROS development" />

For this purpose, we prepared the
[leo_navigation](https://github.com/LeoRover/leo_navigation_tutorial) package
which makes use of many other packages available in ROS to provide autonomous
capabilities. You won't have to write a single line of code, but some
configuration may need to be tweaked to work best in your environment.

To complete this guide, you will need to, first of all, have the IMU and LiDAR
sensors integrated with the system:

{/* TODO link to IMU */}

:::tip

Since Leo 1.8, the IMU is already installed, so you don't need to do this
integration tutorial.

:::

and

{/* TODO link to Hokuyo */}

or

{/* TODO link to Lidar */}

Make sure you are operating on the newest image for the Raspberry Pi and you are
up-to-date with the packages and have the newest firmware flashed.

You will also need to have ROS installed on your computer and some previous
experience with ROS is also recommended.

## Preparing the environment

There are a few things you will need to prepare on Leo Rover and on your
computer, in order to use the software for autonomous navigation.

### On Leo Rover

You will need to build the `leo_navigation` package first. To do this, start by
accessing a remote terminal session on Leo Rover by logging via ssh:

<LinkButton to="/docs/leo-rover/guides/ssh" description="Connect via SSH" />

Create a catkin workspace:

```bash
mkdir -p ~/ros_ws/src
cd ~/ros_ws
catkin config --extend /opt/ros/${ROS_DISTRO}
```

Clone the `leo_navigation` package to the source space:

```bash
cd ~/ros_ws/src
git clone https://github.com/LeoRover/leo_navigation_tutorial.git
```

Use `rosdep` to install dependencies:

```bash
cd ~/ros_ws
rosdep update
rosdep install --from-paths src -iy
```

Build the workspace:

```bash
catkin build
```

Source the devel space:

```bash
source ~/ros_ws/devel/setup.bash
```

:::info

You will have to source the devel space on every terminal session you want to
use the package on. If you want to do it automatically upon logging into a
session, add the above line to the `~/.bashrc` file.

:::

### Checking the sensors

It is also advisable to check whether the sensors are correctly integrated into
the system. Make sure the sensor-related data is published on the correct
topics:

```bash
rostopic echo /imu/gyro
rostopic echo /imu/accel
rostopic echo /imu/mag
rostopic echo /scan
```

You will also need to have the transforms, from `base_link` to frame ids
associated with the sensor messages, available in the tf tree.

```bash
rosrun tf tf_echo base_link imu
rosrun tf tf_echo base_link laser_frame
```

If you see any errors, it probably means you omitted the part about extending
the URDF model when integrating the sensor.

### On your computer

In this guide, your computer will be used for visualizing the data processed on
the rover and for sending navigation goals. For this purpose, you will need to
install the [leo_description](https://github.com/LeoRover/leo_description)
package which contains the robot model and the
[leo_viz](https://github.com/LeoRover/leo_viz) package which contains
configuration files for RViz.

You can build them using the instructions from
[this chapter](ros-development#building-additional-ros-packages). You can also
download the prebuilt packages from the ROS repository by executing:

```bash
sudo apt install ros-${ROS_DISTRO}-leo-viz
```

You will also need to properly configure your computer to communicate in the ROS
network, so be sure you have correctly set these environment variables:

```bash
export ROS_IP="<YOUR_IP>"
export ROS_MASTER_URI="http://10.0.0.1:11311"
```

For more information, visit:

<LinkButton
  to="ros-development#connecting-another-computer-to-ros-network"
  description="ROS development / Connection another computer to ROS network"
/>

## Launching the software

Now that everything is ready, we can proceed to launching some software. This
whole section will describe in detail the functionalities provided in the
`leo_navigation` package and how to use them.

Here's a brief summary of different parts of the navigation software we will
run:

- **Odometry** - It is necessary for any autonomous navigation system to be able
  to estimate its position. For this reason, we will use wheel encoders and IMU
  readings to track the change in position over time.
- **SLAM** - Once we are able estimate the robot's position, we will use it,
  together with the LiDAR sensor measurements, to produce an occupancy map of
  the terrain. While doing so, we will also correct the estimated position based
  on the loop closure detection. After the whole terrain is mapped, we will save
  the map to a file and later use it to track robot's pose against it.
- **Navigation** - With an occupancy map and an accurate enough pose estimation,
  we can finally try to perform some autonomous navigation. When a new
  navigation goal is set, a path planning algorithm will find a collision-free
  path to it. Another algorithm will then send velocity commands to the rover to
  try to follow that path while also being aware of the obstacles.

All of the functions are provided through launch files that are located under
the `launch/` directory. All parameters for the running nodes are loaded from
YAML files located under the `config/` directory.

### Odometry

To estimate the robot's position from wheel encoders and IMU, we will use the
[robot_localization](http://docs.ros.org/melodic/api/robot_localization/html/index.html)
package which contains two state estimation nodes: the `ekf_localization_node`
which implements
[Extended Kalman Filter](https://en.wikipedia.org/wiki/Extended_Kalman_filter)
algorithm and the `ukf_localization_node` which implements
[Unscented Kalman Filter](https://en.wikipedia.org/wiki/Kalman_filter#Unscented_Kalman_filter).
We will use the first option as it is less computationally expensive but the
latter is a more stable alternative.

Without further ado, let's try running the localization software and see the
result. Later, we will explain what is actually going on.

On Leo Rover, type:

```bash
roslaunch leo_navigation odometry.launch
```

On your computer, type:

```bash
roslaunch leo_viz rviz.launch config:=odometry
```

The RViz instance should start and you should see the robot's model. Now, try to
steer the rover (either from the Web UI or by using a joystick). If everything
works, you should see the model moving along the ground plane like in this
video:

<div className="video-container">
  <LiteYouTubeEmbed
    id="bH-mJ7cOwGQ"
    params="autoplay=0&autohide=1&showinfo=0&rel=0"
    poster="hqdefault"
    title="Leo Rover Unboxing and Getting Started"
    webp
  />
</div>

Here's a diagram that illustrates the nodes launched by the `odometry.launch`
file and the connections between them:

<ImageZoom
  src="/img/docs_images/leo-rover/guides/autonomous-navigation/odometry-graph.webp"
  alt="Odometry graph"
  width="2000"
  height="394"
/>

:::info

The ovals represents the nodes. The rectangles with solid lines represent the
topics. The blue text inside the rectangles indicate the type of messages
published on the topic. The arrows between the ovals and rectangles represent
the topic Subscribers and Publishers. The `tf` rectangle represents the
transforms between coordinate frames exchanged through the
[tf2 library](http://wiki.ros.org/tf2).

:::

The state estimation node requires that the input topics are stamped messages
which contain covariance matrices. That's why the `leo_navigation` package
provides the `message_filter` node which:

- for each message on `/wheel_odom` topic, publishes a message on the
  `/wheel_odom_with_covariance` topic which contains the same data but with an
  added covariance matrix,
- for each pair of `/imu/accel` and `/imu/gyro` messages, publishes a message on
  `/imu/data_raw` topic which combines the two messages and adds covariance
  matrices.

Apart from these topics, the `ekf_localization_node` listens for the
`base_link->imu` tf transform to transform IMU readings to a common reference
frame (`base_link` in this case).

The current state estimation is published on the `/odometry/filtered` topic and
the pose is also broadcasted as the `odom->base_link` transform. That's why
having the **Fixed Frame** set to `odom` in RViz, you can see the model moving.

#### 3D odometry

Notice that we didn't use much from the measurements returned by the IMU sensor,
except for the angular velocity which was used to correct the wheel odometry.

Thanks to the [imu_filter_madgwick](http://wiki.ros.org/imu_filter_madgwick)
package, we can get pretty good orientation estimate, which, combined with the
wheel odometry, can be used to track the robot's position in 3D. The package
uses the
[Madgwick's filter](https://x-io.co.uk/open-source-imu-and-ahrs-algorithms/) to
fuse data from a gyroscope, an accelerometer and (optionally) a magnetometer.

To try the robot's state estimation in 3D, start the same launch file as in the
previous example, but with the `three_d` argument set to `true`:

```bash
roslaunch leo_navigation odometry.launch three_d:=true
```

Now, try steering the rover on uneven terrain and observe the result in RViz.

The diagram for the odometry.launch file now changes a little:

<ImageZoom
  src="/img/docs_images/leo-rover/guides/autonomous-navigation/odometry-graph-3d.webp"
  alt="Odometry graph"
  width="2000"
  height="445"
/>

We now have the `/imu_filter_node` which takes messages from the `/imu/data_raw`
topic (gyroscope and accelerometer data), calculates the orientation of the
sensor and publishes the data with added orientation quaternion on the
`/imu/data` topic. The node can also optionally incorporate the `/imu/mag` topic
(magnetometer data).

The `/ekf_localization_node` now uses different configuration which takes the
data from `/imu/data` topic and uses the orientation quaternion in the state
estimation.

#### Configuration

For the `/message_filter` node, there is the **config/message_filter.yaml** file
in which you can change the values on the diagonals of the covariance matrices,
which represent the variances (the uncertainty) of the measurements.

:::warning

It might be better to leave the values as they are if you have no idea what they
mean.

:::

The configuration files for the `/ekf_localization_node` can be found in the
**config/ekf_localization_node/** directory. There are two files:
**ekf_2d.yaml** and **ekf_3d.yaml**. The appropriate file is chosen depending on
the value of 3d argument passed to the launch file.

A full description of parameters for the state estimation node can be found on
the
[robot_localization documentation](http://docs.ros.org/melodic/api/robot_localization/html/state_estimation_nodes.html)
and in the
[ekf config template](https://github.com/cra-ros-pkg/robot_localization/blob/kinetic-devel/params/ekf_template.yaml).

There is also the **config/imu_filter_node.yaml** file for the Madgwick's
filter. The parameters are explained in the
[imu_filter_madgwick wiki page](http://wiki.ros.org/imu_filter_madgwick). If you
want to use the magnetometer data, just change `use_mag` parameter to `true`.

:::warning

It is essential to have the magnetometer properly calibrated for the measured
data to be useful. Magnetometers are prone to distortion, so even a slight
change of the environment can make the current calibration invalid.

:::

If you have set the use_mag parameter and want use the heading information
(compass) in `/ekf_localization_node`, just modify the `imu0_config` parameter
in the **config/ekf_localization_node/ekf_3d.yaml** file, so that the yaw value
is fused into the state estimate:

{/* prettier-ignore */}
```yaml
imu0_config: [false, false, false,
              true,  true,  true,
              false, false, false,
              true,  true,  true,
              true,  false, false]
```

### SLAM

LAM (Simultaneous Localization and Mapping) is a problem of constructing a map
of the environment while simultaneously keeping track of the robot within it. It
is hard because a map is needed for localization and a good pose estimate is
needed for mapping, so this appears to be chicken-and-egg problem. There are,
however, methods of solving it approximately with the use of probabilistic
algorithms, such as the
[particle filter](https://en.wikipedia.org/wiki/Particle_filter).

One such approach, called
[GMapping](https://openslam-org.github.io/gmapping.html), can be used to perform
SLAM with the laser range data (LiDAR scans) and a local odometry source. The
algorithm has its ROS wrapper node in the
[gmapping](http://wiki.ros.org/gmapping) package. To run it, just type:

```bash
roslaunch leo_navigation gmapping.launch
```

:::tip

Mark the start position of the rover! It will be useful later.

:::

On your computer, close any running RViz instance and open a new one with the
`slam` configuration:

```bash
roslaunch leo_viz rviz.launch config:=slam
```

Now try exploring the terrain with your rover and see the map being created. You
will probably experience the model doing discrete jumps on the map from time to
time. This is due to gmapping correcting robot's position within the map. Here's
a video demonstration:

<div className="video-container">
  <LiteYouTubeEmbed
    id="2I3isieU250"
    params="autoplay=0&autohide=1&showinfo=0&rel=0"
    poster="hqdefault"
    title="Leo Rover Unboxing and Getting Started"
    webp
  />
</div>

The diagram for the `gmapping.launch` file is pretty straightforward:

<ImageZoom
  src="/img/docs_images/leo-rover/guides/autonomous-navigation/slam-gmapping-graph.webp"
  alt="Odometry graph"
  width="2000"
  height="804"
/>

The `/slam_gmapping` node takes as the input:

- laser scan data from the LiDAR sensor (`/scan` topic),
- position of the laser reference frame (`base_link->laser_frame` transform),
- current position of the robot from the odometry (`odom->base_link` transform).

As the output, it returns:

- an occupancy grid map of the terrain (`/map` and `/map_metadata` topics),
- current odometry drift based on the estimated position of the robot within the
  map (`map->odom` transform).

:::note

The map, odom and `base_link` are coordinate frame names commonly used in mobile
platforms. To learn about their semantic meaning, you can read
[REP105](https://www.ros.org/reps/rep-0105.html#id8).

When the `/slam_gmapping` node tries to correct robot's position within the map,
it does not broadcas the position as the `map->base_link` transform, because
that would make 2 root coordinate frames (`map` and `odom`). Instead, it
provides the `map->odom` transform which marks the difference between the
odometry position and the actual position of the robot within the map (the
odometry drift).

:::

#### AMCL

If you already explored all the terrain you wanted to map, the `/gmapping` node
can draw unnecessary resources for its mapping part. When map is no longer
updated, it is more efficient to just use an algorithm that will only track the
robot's position against the map you have generated.

That's where the [amcl](http://wiki.ros.org/amcl) package comes in handy. It
implements the
[Monte Carlo localization](https://en.wikipedia.org/wiki/Monte_Carlo_localization)
algorithm for laser range data. To use it, first, save the current map generated
by GMapping using the `map_saver` script:

```bash
rosrun map_server map_saver -f mymap
```

This will create 2 files: **mymap.yaml** containing map's metadata and
**mymap.pgm** containing the actual occupancy grid in binary format.

Close the `gmapping.launch` instance by clicking **Ctrl+C** on the terminal
session it is running on. Then, start `amcl.launch` by typing:

```bash
roslaunch leo_navigation amcl.launch map_file:=<path_to_the_map_file>
```

Replace `<path_to_the_map_file>` with the absolute path to the **mymap.yaml**
file. If the file is in your current working directory, you can instead type:

```bash
roslaunch leo_navigation amcl.launch map_file:=$(realpath mymap.yaml)
```

If your initial pose is far from the real one, AMCL might not be able to
correctly localize the robot on the map. To fix this, you can use the **2D Pose
Estimate** tool in RViz to manually set the initial pose.

When your initial pose is close enough to the real one, try steering the robot
and notice how the position of the robot model in RViz is being corrected by
AMCL.

<ImageZoom
  src="/img/docs_images/leo-rover/guides/autonomous-navigation/amcl.webp"
  alt="Graph using AMCL"
  width="2000"
  height="516"
/>

Similar to the `/slam_gmapping` node, `/amcl` takes the laser range data
(`/scan` topic) as input and provides the odometry drift information by
broadcasting the `map->odom` transform. The difference is that instead of
publishing to the /map topic, it receives the (previously generated) map from
the `/map_server` node.

You can also notice 2 additional topics published by the `/amcl` node:

- `/amcl_pose` - estimated pose of the robot within a map, with covariance,
- `/particlecloud` - a set of pose estimates being maintained by the Monte Carlo
  localization algorithm.

You can visualize the data published on these topics by enabling the AMCL Pose
and/or the `AMCL Particle Cloud` displays in RViz.

#### Configuration

The configuration for the `/slam_gmapping` node is loaded from the
**config/slam_gmapping.yaml** file. For description of each parameter, visit the
[gmapping ROS wiki page](http://wiki.ros.org/gmapping#Parameters).

Here are some parameters you can try to adjust:

- `map_update_interval` - control how often to update the map.
- `linearUpdate`, `angularUpdate`, `temporalUpdate` - control when to process
  new laser scans.
- `particles` - increasing this number may improve the quality of the map at the
  cost of greater computational load.
- `minimumScore` - you can experiment with this parameter if you are
  experiencing jumping pose estimates in large open spaces.
- `maxUrange`, `maxRange` - try to set these parameters like this: `maxUrange` <
  maximum range of your LiDAR sensor \<= `maxRange`.
- `delta` - increasing this will lower the resolution of the map but may improve
  performance.

Similarly, for the `/amcl` node, there is the **config/amcl.yaml** file with the
parameters that are described in the
[amcl ROS wiki page](http://wiki.ros.org/amcl#Parameters).

Here are some parameters worth noticing:

- `min_particles`, `max_particles` - higher number of particles may increase the
  accuracy of the estimates at the cost of computational load.
- `update_min_d`, `update_min_a` - control when to perform filter updates.
- `initial_pose_x`, `initial_pose_y`, `initial_pose_a` - initial mean pose used
  to initialize the filter.
- `laser_max_beams` - you can experiment with this parameter. Higher number may
  increase the accuracy at the cost of computational load.

:::warning

Changing the parameters in a way that results in higher computational load may
lead to violation of the real-time constraint. This in turn can make the
algorithms perform even worse.

:::

### Navigation
